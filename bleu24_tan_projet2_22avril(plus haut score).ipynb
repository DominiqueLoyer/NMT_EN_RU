{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPmjmrQj8N1ssUotrJzPTdD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BjqzZ2k6kXq4"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","DIC-9345 - Projet 2: Traduction Automatique Neuronale (TAN) - EN->RU\n","Version utilisant NLLB-600M pour viser un meilleur score.\n","Entraînement sur opus_books complet, 5 époques.\n","Utilise do_eval=False. Batch size 4.\n","Correction: Modification de la stratégie de sauvegarde pour économiser l'espace disque.\n","\"\"\"\n","\n","# @title 1. Installation des bibliothèques nécessaires\n","# Installe les bibliothèques Hugging Face (transformers, datasets), SacreBLEU et Accelerate.\n","# Sentencepiece est nécessaire pour le tokenizer NLLB.\n","!pip install transformers[torch] datasets sacrebleu accelerate evaluate sentencepiece -q\n","\n","print(\"Installation terminée.\")\n","\n","# @title 2. Importations et Configuration Initiale\n","import os\n","# Silence XLA/TensorFlow CUDA warnings\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n","# Désactive Weights & Biases (si non utilisé)\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","import torch\n","import numpy as np\n","# Utilisation de 'evaluate' au lieu de 'load_metric' pour les métriques\n","from datasets import load_dataset\n","import evaluate # Nouvelle façon de charger les métriques\n","from transformers import (\n","    AutoTokenizer, # Utilisation de AutoTokenizer pour NLLB\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer\n",")\n","\n","# Configuration pour Anglais -> Russe\n","# CHANGEMENT: Utilisation du modèle NLLB-600M\n","MODEL_CHECKPOINT = \"facebook/nllb-200-distilled-600M\"\n","# Codes langues pour NLLB (peuvent différer des codes Helsinki)\n","# Voir: https://huggingface.co/facebook/nllb-200-distilled-600M#languages-covered\n","SOURCE_LANG_NLLB = \"eng_Latn\" # Code NLLB pour Anglais\n","TARGET_LANG_NLLB = \"rus_Cyrl\" # Code NLLB pour Russe\n","\n","# Noms des colonnes dans le dataset opus_books\n","SOURCE_LANG_DATA = \"en\"\n","TARGET_LANG_DATA = \"ru\"\n","\n","DATASET_NAME = \"opus_books\"\n","DATASET_CONFIG = \"en-ru\"\n","\n","# Limites pour l'exemple (COMMENTÉES POUR UTILISER TOUTES LES DONNÉES)\n","# MAX_TRAIN_SAMPLES = 10000\n","# MAX_VAL_SAMPLES = 1000\n","# MAX_TEST_SAMPLES = 1000\n","MAX_INPUT_LENGTH = 128 # Peut nécessiter ajustement pour NLLB si les phrases sont longues\n","MAX_TARGET_LENGTH = 128\n","\n","# Vérification explicite du device GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Utilisation du périphérique : {device}\")\n","if torch.cuda.is_available():\n","    print(f\"Nom du GPU: {torch.cuda.get_device_name(0)}\")\n","else:\n","    print(\"Aucun GPU détecté, utilisation du CPU.\")\n","\n","\n","# @title 3. Chargement et Prétraitement des Données\n","\n","# Charger le jeu de données (opus_books en-ru)\n","try:\n","    raw_datasets_full = load_dataset(DATASET_NAME, DATASET_CONFIG)\n","    print(f\"Dataset {DATASET_NAME} ({DATASET_CONFIG}) chargé.\")\n","    print(raw_datasets_full)\n","    # Division du split 'train'\n","    train_test_split = raw_datasets_full['train'].train_test_split(test_size=0.05, seed=42)\n","    train_val_split = train_test_split['train'].train_test_split(test_size=0.05, seed=42)\n","\n","    raw_datasets = {\n","        'train': train_val_split['train'],\n","        'validation': train_val_split['test'],\n","        'test': train_test_split['test']\n","    }\n","    print(\"Dataset divisé en train/validation/test.\")\n","    print(f\"Tailles réelles des splits - Train: {len(raw_datasets['train'])}, Validation: {len(raw_datasets['validation'])}, Test: {len(raw_datasets['test'])}\")\n","\n","except Exception as e:\n","    print(f\"Erreur lors du chargement ou de la division du dataset {DATASET_NAME} ({DATASET_CONFIG}): {e}\")\n","    raise e\n","\n","# Charger le tokenizer NLLB\n","# Utilisation de AutoTokenizer, spécification des langues source/cible\n","tokenizer = AutoTokenizer.from_pretrained(\n","    MODEL_CHECKPOINT,\n","    src_lang=SOURCE_LANG_NLLB,\n","    tgt_lang=TARGET_LANG_NLLB\n",")\n","print(f\"Tokenizer chargé pour {MODEL_CHECKPOINT} (src={SOURCE_LANG_NLLB}, tgt={TARGET_LANG_NLLB})\")\n","\n","# Fonction de prétraitement adaptée pour NLLB\n","def preprocess_function(examples):\n","    # Utiliser les clés du dataset ('en', 'ru')\n","    inputs = [ex[SOURCE_LANG_DATA] for ex in examples[\"translation\"]]\n","    targets = [ex[TARGET_LANG_DATA] for ex in examples[\"translation\"]]\n","\n","    # Tokenisation des entrées (Anglais)\n","    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n","\n","    # Tokenisation des sorties (Russe) comme labels\n","    labels = tokenizer(text_target=targets, max_length=MAX_TARGET_LENGTH, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","# Appliquer le prétraitement aux datasets\n","num_cpus = os.cpu_count()\n","print(f\"Utilisation de {num_cpus} coeurs pour le prétraitement.\")\n","tokenized_datasets = {}\n","for split, dataset in raw_datasets.items():\n","     cols_to_remove = ['id', 'translation']\n","     tokenized_datasets[split] = dataset.map(\n","         preprocess_function,\n","         batched=True,\n","         remove_columns=cols_to_remove,\n","         num_proc=num_cpus,\n","         desc=f\"Tokenizing {split} split...\"\n","     )\n","\n","print(\"Prétraitement terminé.\")\n","print(\"Structure après tokenisation (exemple train):\", tokenized_datasets[\"train\"])\n","\n","\n","# Utiliser les datasets complets\n","train_dataset = tokenized_datasets[\"train\"]\n","eval_dataset = tokenized_datasets[\"validation\"]\n","test_dataset_tokenized = tokenized_datasets[\"test\"]\n","test_dataset_raw = raw_datasets[\"test\"]\n","\n","print(f\"Taille du jeu d'entraînement utilisé: {len(train_dataset)} exemples\")\n","print(f\"Taille du jeu d'évaluation utilisé (référence): {len(eval_dataset)} exemples\")\n","print(f\"Taille du jeu de test utilisé: {len(test_dataset_tokenized)} exemples (tokenisé)\")\n","print(f\"Taille du jeu de test utilisé: {len(test_dataset_raw)} exemples (raw)\")\n","\n","\n","# @title 4. Chargement du Modèle et Configuration de l'Entraînement\n","\n","# Charger le modèle NLLB\n","model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n","model.to(device)\n","print(f\"Modèle {MODEL_CHECKPOINT} chargé et envoyé sur {device}.\")\n","\n","# Nom du run pour le suivi\n","run_name = f\"nllb-600m-finetuned-{SOURCE_LANG_DATA}-to-{TARGET_LANG_DATA}-opus-full-5e-v2\" # Run NLLB v2\n","\n","# Arguments d'entraînement\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=run_name,\n","    do_train=True,\n","    do_eval=False,              # PAS d'évaluation pendant l'entraînement\n","    logging_strategy=\"steps\",\n","    logging_steps=100,\n","    save_strategy=\"no\",         # CORRECTION: Ne pas sauvegarder pendant l'entraînement\n","    learning_rate=5e-6,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    gradient_accumulation_steps=4,\n","    weight_decay=0.01,\n","    # save_total_limit=2,       # Non pertinent si save_strategy=\"no\"\n","    num_train_epochs=5,\n","    predict_with_generate=True,\n","    fp16=torch.cuda.is_available(),\n","    push_to_hub=False,\n","    generation_max_length=MAX_TARGET_LENGTH,\n","    # report_to=\"wandb\",\n",")\n","\n","# Data Collator (standard)\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","# Métriques d'évaluation\n","sacrebleu_metric = evaluate.load(\"sacrebleu\")\n","chrf_metric = evaluate.load(\"chrf\")\n","\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","    return preds, labels\n","\n","# compute_metrics (inchangé, mais sera appelé par predict)\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","\n","    # Ignorer les tokens de padding (-100) avant décodage\n","    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Nettoyage simple\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    try:\n","        bleu_result = sacrebleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n","        # Calcul chrF standard\n","        chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n","        result = {\"bleu\": bleu_result[\"score\"], \"chrf\": chrf_result[\"score\"]}\n","    except Exception as e:\n","        print(f\"Erreur lors du calcul des métriques: {e}\")\n","        print(\"Prédictions:\", decoded_preds[:2]) # Afficher les 2 premières pour débogage\n","        print(\"Labels:\", decoded_labels[:2])\n","        result = {\"bleu\": 0.0, \"chrf\": 0.0} # Retourner 0 en cas d'erreur\n","\n","    # Calcul longueur moyenne (optionnel)\n","    try:\n","        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","        result[\"gen_len\"] = np.mean(prediction_lens)\n","    except:\n","        result[\"gen_len\"] = 0 # Gérer le cas où preds est vide ou autre erreur\n","\n","    result = {k: round(v, 4) for k, v in result.items()}\n","    return result\n","\n","# Initialiser le Trainer\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","print(f\"Configuration de l'entraînement terminée ({run_name}, prêt pour GPU, do_eval=False).\")\n","\n","\n","# @title 5. Lancement de l'Entraînement (Fine-tuning)\n","# ATTENTION: Ceci prendra BEAUCOUP plus de temps (plusieurs heures / toute la nuit).\n","print(f\"Début de l'entraînement ({run_name}) sur GPU...\")\n","try:\n","    train_result = trainer.train()\n","    # Sauvegarde du modèle final uniquement APRES la fin de l'entraînement\n","    print(\"Entraînement terminé. Sauvegarde du modèle final...\")\n","    trainer.save_model()\n","    print(\"Modèle final sauvegardé.\")\n","    # Log et sauvegarde des métriques d'entraînement\n","    metrics = train_result.metrics\n","    metrics[\"train_samples\"] = len(train_dataset)\n","    trainer.log_metrics(\"train\", metrics)\n","    trainer.save_metrics(\"train\", metrics)\n","    trainer.save_state() # Sauvegarde l'état du trainer (utile pour reprise éventuelle)\n","\n","except Exception as e:\n","    print(f\"Une erreur est survenue pendant l'entraînement : {e}\")\n","    if \"CUDA out of memory\" in str(e):\n","        print(\"Erreur 'CUDA out of memory'. Essayez de réduire 'per_device_train_batch_size' ou 'gradient_accumulation_steps'.\")\n","    # Afficher l'erreur spécifique si c'est celle vue précédemment\n","    if \"enforce fail at inline_container.cc\" in str(e):\n","        print(\"Erreur interne PyTorch/XLA détectée pendant l'entraînement.\")\n","    print(\"L'entraînement a été interrompu.\")\n","    # Essayer de sauvegarder le dernier état même si erreur\n","    try:\n","        print(\"Tentative de sauvegarde du dernier état du modèle...\")\n","        trainer.save_model(os.path.join(run_name, \"checkpoint-interrupted\"))\n","        trainer.save_state(os.path.join(run_name, \"checkpoint-interrupted\"))\n","        print(\"Dernier état sauvegardé dans 'checkpoint-interrupted'.\")\n","    except Exception as save_e:\n","        print(f\"Impossible de sauvegarder le dernier état après interruption: {save_e}\")\n","\n","\n","# @title 6. Évaluation sur le Jeu de Test\n","# S'assurer que le modèle est chargé (soit le dernier après entraînement complet, soit depuis un checkpoint si interrompu)\n","# Si l'entraînement a été interrompu, il faudrait manuellement charger le checkpoint sauvegardé avant predict.\n","# Pour l'instant, on suppose que l'entraînement s'est terminé ou que le modèle dans 'trainer' est utilisable.\n","\n","print(\"Début de l'évaluation finale sur le jeu de test avec le modèle actuel...\")\n","model.eval()\n","\n","try:\n","    # Utiliser trainer.predict() pour l'évaluation finale sur le jeu de test\n","    predict_results = trainer.predict(test_dataset_tokenized, metric_key_prefix=\"test\")\n","\n","    metrics = predict_results.metrics\n","    metrics[\"test_samples\"] = len(test_dataset_raw)\n","\n","    print(f\"----- Résultats de l'évaluation finale sur le jeu de test ({run_name}) -----\")\n","    print(f\"Score SacreBLEU: {metrics.get('test_bleu', 'N/A'):.4f}\")\n","    print(f\"Score chrF: {metrics.get('test_chrf', 'N/A'):.4f}\")\n","\n","    # Recalculons pour avoir tous les détails de SacreBLEU si nécessaire\n","    if predict_results.predictions is not None:\n","        preds = predict_results.predictions\n","        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n","        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","        # Références du dataset brut\n","        references = [ex['translation'][TARGET_LANG_DATA] for ex in test_dataset_raw]\n","        cleaned_preds, cleaned_labels = postprocess_text(decoded_preds, references)\n","\n","        try:\n","            final_bleu_metric = evaluate.load(\"sacrebleu\")\n","            test_bleu_results_detailed = final_bleu_metric.compute(predictions=cleaned_preds, references=cleaned_labels)\n","\n","            print(f\"(Recalculé) Score SacreBLEU: {test_bleu_results_detailed['score']:.4f}\")\n","            if 'precisions' in test_bleu_results_detailed:\n","                print(f\"Précisions BLEU (1-4 grams): { [round(p, 4) for p in test_bleu_results_detailed['precisions']] }\")\n","            print(f\"Ratio de brièveté (BP): {test_bleu_results_detailed.get('bp', 'N/A'):.4f}\")\n","            print(f\"Longueur moyenne des prédictions: {np.mean([len(p.split()) for p in cleaned_preds]):.2f} mots\")\n","            print(f\"Longueur moyenne des références: {np.mean([len(l[0].split()) for l in cleaned_labels]):.2f} mots\")\n","\n","            # Ajout des détails au dictionnaire de métriques\n","            metrics[\"test_bleu_precisions\"] = test_bleu_results_detailed.get('precisions')\n","            metrics[\"test_bp\"] = test_bleu_results_detailed.get('bp')\n","\n","            # Sauvegarde des prédictions/références\n","            output_prediction_file = os.path.join(run_name, \"test_predictions_ru.txt\")\n","            output_reference_file = os.path.join(run_name, \"test_references_ru.txt\")\n","            with open(output_prediction_file, \"w\", encoding=\"utf-8\") as writer:\n","                writer.write(\"\\n\".join(cleaned_preds))\n","            with open(output_reference_file, \"w\", encoding=\"utf-8\") as writer:\n","                writer.write(\"\\n\".join([ref[0] for ref in cleaned_labels]))\n","            print(f\"Prédictions sauvegardées dans: {output_prediction_file}\")\n","            print(f\"Références sauvegardées dans: {output_reference_file}\")\n","\n","        except Exception as e:\n","            print(f\"Erreur lors du recalcul détaillé de BLEU ou de la sauvegarde des fichiers: {e}\")\n","            # Vérifier si c'est l'erreur d'espace disque\n","            if isinstance(e, OSError) and e.errno == 28:\n","                 print(\"ERREUR: Plus d'espace disque disponible pour sauvegarder les prédictions/références.\")\n","            # Sauvegarder quand même les métriques principales si possible\n","\n","\n","    # Commentaire de log_metrics pour éviter TypeError potentiel\n","    # trainer.log_metrics(\"test\", metrics)\n","    # Sauvegarder les métriques finales (tentative même si erreur disque avant)\n","    try:\n","        trainer.save_metrics(\"test\", metrics)\n","        print(\"Métriques de test sauvegardées.\")\n","    except Exception as save_e:\n","        print(f\"Impossible de sauvegarder les métriques de test: {save_e}\")\n","        if isinstance(save_e, OSError) and save_e.errno == 28:\n","             print(\"ERREUR: Plus d'espace disque disponible pour sauvegarder les métriques.\")\n","\n","except Exception as pred_e:\n","    print(f\"Une erreur est survenue pendant la prédiction/évaluation : {pred_e}\")\n","\n","\n","print(\"Évaluation finale terminée (ou tentée).\")\n","\n","\n","# @title 7. Exemple d'Inférence (Traduction d'une phrase EN->RU)\n","# Essayer l'inférence même si l'évaluation a eu des soucis\n","try:\n","    print(\"\\nExemple d'inférence avec le modèle NLLB fine-tuné...\")\n","    sentence_en = \"Machine translation is fascinating.\"\n","    print(f\"Phrase source ({SOURCE_LANG_DATA}): {sentence_en}\")\n","\n","    # Tokenisation pour NLLB\n","    inputs = tokenizer(sentence_en, return_tensors=\"pt\").to(device)\n","\n","    # Spécifier la langue cible pour la génération avec NLLB\n","    forced_bos_token_id = tokenizer.lang_code_to_id[TARGET_LANG_NLLB]\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            forced_bos_token_id=forced_bos_token_id,\n","            max_length=MAX_TARGET_LENGTH,\n","            num_beams=4,\n","            early_stopping=True\n","        )\n","\n","    translation_ru = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    print(f\"Traduction ({TARGET_LANG_DATA}): {translation_ru}\")\n","\n","    # Autre exemple\n","    sentence_en_2 = \"This model was fine-tuned on the full opus_books dataset.\"\n","    print(f\"\\nPhrase source ({SOURCE_LANG_DATA}): {sentence_en_2}\")\n","    inputs_2 = tokenizer(sentence_en_2, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        outputs_2 = model.generate(\n","            **inputs_2,\n","            forced_bos_token_id=forced_bos_token_id,\n","            max_length=MAX_TARGET_LENGTH,\n","            num_beams=4,\n","            early_stopping=True\n","        )\n","    translation_ru_2 = tokenizer.decode(outputs_2[0], skip_special_tokens=True)\n","    print(f\"Traduction ({TARGET_LANG_DATA}): {translation_ru_2}\")\n","\n","except Exception as inf_e:\n","    print(f\"Une erreur est survenue pendant l'inférence: {inf_e}\")\n","\n","\n","print(f\"\\nScript ({run_name}) terminé pour EN->RU (GPU).\")\n","\n"]}]}